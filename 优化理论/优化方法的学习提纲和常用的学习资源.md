#### 优化理论主要内容和参考书
- [ ]  1.首先应该了解凸集相关的性质　特别是凸集分离定理　凸集的支撑面等性质，两个不等式系统有解无解的择一性（ferkas引理和gordan引理），以及相关变形形式的结论，这些结论对后续导出有约束问题的KKT条件至关重要。
这部分可以参考　

《convex optimazation》    

《nonlinear programming Theory and Algorithms》

- [ ] 2.了解凸函数性质，重点如何判断凸函数（定义法　，上镜图方法　可导条件下的一阶二阶方法　）　和保凸运算的相关性质（非负加权　复合仿射映射 逐点最大和逐点上确界　复合规则　透视函数等等）；共轭函数的相关性质，特别是一些凸函数的共轭函数的简单求法，这里比较难以理解的是向量的共轭范数，共轭函数在lagrange对偶函数的求解过程中会扮演十分重要的作用，也是因为共轭函数本身的凸性，也因为有些函数的共轭函数可能求解起来比本身更加简便。
　

《convex optimazation》  
《Nolinear Programming》
《First-Order Methods in Optimization》  
《convex Analysis and optimazation》　　
《Foundations of optimization 》

- [ ] ３．凸优化问题的等价的等价问题变形，这个至关重要。　典型的凸优化问题：线性规划问题　　二次规划问题　　几何规划问题。特别是等价问题这个十分重要，这部分的核心并不是搞明白如何去设计算法对上面的问题进行求解，重点是把搞清楚平日里遇到的问题可以归为哪一类问题进行求解或者说转化

《convex optimazation》 

- [ ] ４．对于带约束问题主要有两种求解范式，一个是在可导的保证下，利用最优性的必要条件KKT条件，直接求解KKT条件，获得局部最优点的可能点，然后根据最优性的二阶必要条件和二阶充分条件做断定这些点是否时局部最优点。当然我们知道对于凸问题来说，KKT是一阶充要条件。但是对于一些非线性等式约束或者一些非凸的约束问题来说，可以通过KKT条件求出可能的候选点，然后基于最优性的二阶必要和充分条件来依次断定这些点是不是局部最优点，实际上求取KKT点是十分困难的，这个方法能应对的范围是十分有限的，一般来说，几个约束条件和几个变量的时候可用；第二中范式主要时通过lagrange对偶来求解，并不直接通过KKT必要条件来求出KKT点，而是先求lagrange对偶函数，然后对lagrange对偶函数求取极大值，即对偶问题，利用对偶问题的最优值是原问题的的最优值下估计这么个特性来进行处理，特别重要的是，对于凸问题来说，在满足slater 条件的情况下，强对偶会成立，即对偶问题的最优值会等于原问题的最优值，并且对偶问题的解
和原问题的解会满足KKT关系式，，所以常常会先求对偶函数，然后求解对偶问题，当然这里也有很好的性质保证，那就是对偶问题的本身的凸性保障。

对偶成立的几何解释和鞍点解释都是十分重要的特性，需要弄清楚。


这里学习的难点实际上是数学分析的相关技巧

《nonlinear programming Theory and Algorithms》　　
《linear and nonlinear programing》（这本书感觉有一些错误）
《Numerical Optimization 2ed - Nocedal》    

《Foundations of optimization 》
《convex optimazation》 

找一本书作为重心，其他的书翻阅补充，对比即可，这个也是学习的核心，每本书实际上写的东西差不多的，一本通，本本通。




- [ ] 5.无约束问题的的最优性条件,凸优化的最优性条件会有很大的简便性 和可导条件下无约束问题的相关算法  
《An Introduction to Optimization》（最优化导论）
《Numerical Optimization 2ed - Nocedal》  
《nonlinear programming Theory and Algorithms》
《最优化理论与算法》  


- [ ] 6．有约束问题的求解算法　　　　

障碍法　内点法　原始－对偶内点法等等

《Numerical Optimization 2ed - Nocedal》  
《nonlinear programming Theory and Algorithms》
《convex optimazation》  
《Nolinear Programming》


- [ ] 7．（重点）凸优化的应用，这个特别重要，特别是在机器学习和控制学科上的应用，这个是我们学习本课程的核心目的。最好在学习完这个部分后，能给出一些机器学习有关的优化问题。

逼近与拟合
统计估计
几何问题

《convex optimazation》 

- [ ] 8.在没有可导保证下的无约束问题求解的现代方法　　

　subgradient　　
  proximal gradient descent　　
  stochastic gradient descent　　

《First-Order Methods in Optimization》

《Lectures on Convex　Optimization》
http://www.stat.cmu.edu/~ryantibs/convexopt/  
https://web.stanford.edu/class/ee364b/lectures.html  
http://www.seas.ucla.edu/~vandenbe/ee236c.html  


- [ ] ９．高级课题　　

有约束和无约束问题的现代方法
《Lectures on Convex　Optimization》


《Nolinear Programming》

http://www.stat.cmu.edu/~ryantibs/convexopt/  
《First-Order Methods in Optimization》 
特别重要的是对偶分解　　ＡＤＭＭ等算法

- [ ] 10.习题

《convex optimazation》 
《nonlinear programming Theory and Algorithms》　　
《An Introduction to Optimization》


- [ ] 11．经典的问题

- SVM
- 最大熵模型/逻辑回归　　
- least square regression
- mlp
- lasso
- ridge regression
- PCA
- LDA（线性判别分析）
- K-means
- ＥＭ算法
- AdaBoost
- gradient boosting
- 生成对抗网络
- 神经网络的相关优化方法
- Auto-ML

- [ ] 12.数学基础

核心是数学分析　可能会有一点拓扑相关的东西

线性代数和矩阵分析是比较常用的技巧

泛函分析特别重要,主要要建立不一样的分析思路，一些泛函分析的观点后续很重要

概率论虽然很重要，但是相对简单，可能在设计具体的机器学习算法的时候会用到



####  机器学习/优化理论国外相关课程  
http://www.seas.ucla.edu/~vandenbe/ee236c.html  
http://www.seas.ucla.edu/~vandenbe/ee236b/ee236b.html  
http://stanford.edu/class/ee364a/    
https://web.stanford.edu/class/ee364b/lectures.html  
http://www.stat.cmu.edu/~ryantibs/convexopt/   
https://www.stat.cmu.edu/~ryantibs/convexopt-F13/  
http://www.mop.uni-saarland.de/teaching/CAO18/index.shtml  
http://homepage.divms.uiowa.edu/~atkinson/ena_master.html  
https://www.cis.upenn.edu/~cis515/   
https://www.cis.upenn.edu/~cis515/cis515-notes-20.html  
https://davidrosenberg.github.io/ml2019/#home  
https://www.ml.cmu.edu/academics/classes.html  
http://www.cs.cmu.edu/~./213/  
http://alex.smola.org/teaching/cmu2013-10-701/  
https://www.cs.cmu.edu/~epxing/Class/10715/   
http://alex.smola.org/teaching/10-701-15/  
http://www.stat.cmu.edu/~larry/=sml/  
http://www.stat.cmu.edu/~larry/=stat705/  
http://curtis.ml.cmu.edu/w/courses/index.php/Machine_Learning_with_Large_Datasets_10-605_in_Fall_2017  
http://bicmr.pku.edu.cn/~wenzw/optbook/pages/contents/contents.html  
http://bicmr.pku.edu.cn/~wenzw/opt-2020-fall.html  
http://bicmr.pku.edu.cn/~wenzw/bigdata2020.html